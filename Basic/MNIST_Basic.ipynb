{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsh/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "import time\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "\n",
    "mnist_train = mx.gluon.data.vision.MNIST(train=True, transform=transform)\n",
    "mnist_test = mx.gluon.data.vision.MNIST(train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1) 5.0\n"
     ]
    }
   ],
   "source": [
    "data, label = mnist_train[0]\n",
    "print(data.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "# (height, width, channel)\n",
    "image = mx.nd.tile(data, (1,1,3)) # broadcast single channel to 3\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFOEHgiL8QFCEHwiK8ANBEX4gKMIPBPUf/Iqa+Y/vp7oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1524fafcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image.asnumpy()) # label : 5\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_examples = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = mx.gluon.data.DataLoader(mnist_train, batch_size, shuffle = True)\n",
    "test_data = mx.gluon.data.DataLoader(mnist_test, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 1) (64,)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_data:\n",
    "    print(data.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "W = nd.random_normal(shape = (num_inputs, num_outputs), ctx = model_ctx)\n",
    "b = nd.random_normal(shape = num_outputs, ctx = model_ctx)\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define softmax activation function\n",
    "def softmax(y):\n",
    "    exp = nd.exp(y - nd.max(y, axis = 1).reshape((-1, 1)))\n",
    "    norms = nd.sum(exp, axis = 1).reshape((-1, 1))\n",
    "    return exp / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model 'net'\n",
    "def net(x):\n",
    "    y = nd.dot(x, W) + b\n",
    "    yhat = softmax(y)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define cross-entropy loss function\n",
    "def cross_entropy(y, yhat):\n",
    "    return -nd.sum(y * nd.log(yhat + 1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define SGD optimizer\n",
    "def SGD(params, learning_rate):\n",
    "    for param in params:\n",
    "        nd.elemwise_add(-learning_rate * param.grad, param, out = param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define accuracy metric\n",
    "def evaluate_accuracy(dataset, net):\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    for data, label in dataset:\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        output = net(data)\n",
    "        pred = nd.argmax(output, axis = 1)\n",
    "        correct += nd.sum(pred == label)\n",
    "        total += data.shape[0]\n",
    "    return (correct / total).asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 4.40526199532, Train_acc 0.736133, Test_acc 0.7341\n",
      "Epoch 1. Loss: 3.6046412262, Train_acc 0.742133, Test_acc 0.7409\n",
      "Epoch 2. Loss: 3.51621165072, Train_acc 0.749883, Test_acc 0.7449\n",
      "Epoch 3. Loss: 3.47068470256, Train_acc 0.754133, Test_acc 0.749\n",
      "Epoch 4. Loss: 3.43547365608, Train_acc 0.756983, Test_acc 0.7505\n",
      "Epoch 5. Loss: 3.41347859523, Train_acc 0.7578, Test_acc 0.752\n",
      "Epoch 6. Loss: 3.38642058849, Train_acc 0.756383, Test_acc 0.7513\n",
      "Epoch 7. Loss: 3.37197168465, Train_acc 0.7589, Test_acc 0.7511\n",
      "Epoch 8. Loss: 3.35738005021, Train_acc 0.7607, Test_acc 0.7514\n",
      "Epoch 9. Loss: 3.34331607386, Train_acc 0.759867, Test_acc 0.7524\n",
      "--- 146.4991009235382 seconds ---\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for data, label in train_data:\n",
    "        data = data.as_in_context(model_ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % \\\n",
    "          (i, cumulative_loss/num_examples, train_accuracy, test_accuracy))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Dense(num_outputs)\n",
    "net0 = gluon.nn.Dense(num_outputs, in_units = 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense(None -> 10, linear)\n",
      "<bound method Block.collect_params of Dense(None -> 10, linear)>\n"
     ]
    }
   ],
   "source": [
    "print(net)\n",
    "print(net.collect_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense(784 -> 10, linear)\n",
      "<bound method Block.collect_params of Dense(784 -> 10, linear)>\n"
     ]
    }
   ],
   "source": [
    "print(net0)\n",
    "print(net0.collect_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter initialize (standard normal dist.)\n",
    "# NOT actually initialize any parameters\n",
    "# Just make a note of which initializer to associate with each parameter\n",
    "net.collect_params().initialize(mx.init.Normal(sigma = 1.0), ctx = model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 36.39788055  10.23013878  -7.5571928   47.47077179  34.09246063\n",
       "   11.50991631 -24.1760025    1.69304657  28.10068512  12.82897377]\n",
       " [  8.98277283 -60.89109039 -15.04196739  -7.84921455  -6.66406155\n",
       "  -27.89904404   7.30398369 -37.76194763  88.00669098  -9.44850731]\n",
       " [-12.53927326  19.12228966  24.60320663  -4.40116119   3.49795532\n",
       "  -15.99965954   3.34893417  -8.46943951   5.25111389  12.71951866]\n",
       " [-34.35363388  56.54007721  16.3698101    1.33660984  -7.52118301\n",
       "   -4.46886539  14.86792564  52.2303009   80.30264282   0.3146534 ]\n",
       " [ -9.06959724  27.68821907 -37.49703979  15.88933563 -20.37697792\n",
       "  -37.50652695 -36.64508057 -45.8342247  -20.86832047  17.66373444]\n",
       " [ 37.02320862   1.10792112  49.18914795  26.6259079   -4.20714331\n",
       "   -2.13836193  13.25164986   8.5969162  -14.51876354  42.27854919]\n",
       " [ 15.37509632 -56.44948196 -38.88101196 -10.55745029 -12.51163673\n",
       "  -16.66604996  28.01202393  38.92884064  14.82570171  -4.33174515]\n",
       " [-38.45901489 -44.24649811  21.15582466   0.30208349  38.21515656\n",
       "    7.74401093 -26.35870934 -35.10245895 -13.43715954 -35.67159271]\n",
       " [ 21.42147255 -44.94487     57.92778778  -5.67542696 -29.83460808\n",
       "  -47.60393524  18.52421951   1.387393    38.57361221 -58.91785049]\n",
       " [-21.690979   -19.47332764  36.17378998  10.34971619  -6.64043427\n",
       "  -55.07891083   2.81403446 -17.92449951  45.22028351  62.3012352 ]\n",
       " [-30.20979309  29.6397953  -23.292696   -39.40796661   7.9430933\n",
       "    9.07522297  -1.08002853 -14.4789505   17.63011551 -24.47132874]\n",
       " [ 36.54386139  -3.62511349 -25.88479996 -26.71583939 -21.64933395\n",
       "  -21.02140045  -0.29542923 -19.90966988 -27.02272034 -15.40571213]\n",
       " [  4.87643051   4.16219711 -18.09184837  25.19689178  -4.97790289\n",
       "  -13.47163391  22.80561066  -5.80659676 -14.02758598  -1.40292263]\n",
       " [  0.61572409 -29.39112663 -42.54616928  40.28364563  17.60425186\n",
       "   12.55186939   8.56111145  30.42233849 -12.25804996  22.24075508]\n",
       " [ 51.8593483  -25.85593033   2.06684685  -2.521698     7.97913408\n",
       "  -15.22157097  30.4724369  -17.28086662 -15.87079525 -37.88763428]\n",
       " [ 20.16454887  32.67378616  54.93113708 -32.88900375  12.59731007\n",
       "   19.36335945  43.22251511  -8.09384155  -3.15726995  -1.53949738]\n",
       " [ 36.18852234  32.78542328 -26.13380623   3.7723968  -19.80328751\n",
       "  -29.94614792  21.03368759  11.18525887  37.37220383  49.29147339]\n",
       " [-26.00497627 -28.37539673  -2.45050621  -7.90806866 -15.65933228\n",
       "  -31.39964676   0.44562149  -6.69574928  44.58082581 -32.12588501]\n",
       " [-19.65372086 -12.20156956  55.03089905  39.93519592 -17.439888\n",
       "   10.25329399 -21.6372261  -22.80717659  35.33465576 -48.10915375]\n",
       " [ 66.88043976   3.49958229  15.36491013  16.14625168 -37.18071747\n",
       "  -13.33537674 -32.91561127  47.56060791 -29.7415657    9.35262489]\n",
       " [-48.81437683   1.75275517  58.7802887  -24.12527275 -11.26941586\n",
       "    3.89956284 -62.86769867  -9.27966118 -10.60273933  64.39085388]\n",
       " [-34.11316681  26.87935638   7.76569319 -13.27520943 -64.0185318\n",
       "   35.19230652  11.38866997   7.80380058  11.30923653 -32.06341553]\n",
       " [-48.64292526  47.83629608 -30.39716911 -30.90102577 -34.92622757\n",
       "    9.98014832  -7.76119041  53.12503052  -4.50998878 -64.35865021]\n",
       " [-21.30739975  -9.38572693  27.5004673  -58.85189056 -64.27549744\n",
       "   -3.01312447   3.46550226 -76.2338028   42.9230957    2.60678601]\n",
       " [-13.75109673   5.36846542 -11.04444122  30.74976921 -38.8914566\n",
       "  -48.57840729   2.69056606  39.66944504  28.01870346  19.12059593]\n",
       " [ 28.92050362  51.53560638 -14.01418114   2.97013283 -48.13302612\n",
       "   34.70594025   4.31308222 -18.11044884  -8.16634369  27.01631355]\n",
       " [-26.33234787  50.32277679 -33.94463348  20.24856567 -14.45377064\n",
       "   46.50502014  16.22203636   7.10617065  32.50584412  16.94532776]\n",
       " [  0.84329736 -23.03492355 -43.22175598 -30.98718834  49.47893906\n",
       "  -73.24240112 -26.89916801 -27.06583405  16.24169159  34.81882858]\n",
       " [-29.00318527  21.48453331  19.78822708 -22.21197891 -48.58602524\n",
       "   -4.73141956   2.48611259 -16.63925171   5.61221695  13.43535042]\n",
       " [ -8.11971283 -13.05194569  16.54127693  61.35912704 -20.26457977\n",
       "   37.01662827 -12.11838245 -27.73227692  -0.96251011  13.48583126]\n",
       " [-19.46736145  -0.62307167  11.41048813   2.72582626  31.02744484\n",
       "   31.12277031  -4.66431141  -6.51179695  -8.81617165  76.65869141]\n",
       " [  5.70259476  24.45142174  -9.14840317  46.78833008 -44.64673996\n",
       "    8.02528381  36.55521393  39.46034622 -34.43409348 -30.21869659]\n",
       " [ 38.29279709  13.91466904  38.86516571  38.29637146 -19.12198639\n",
       "  -22.69861794  54.3019104  -30.22833252  20.5669117  -48.39012909]\n",
       " [ 21.76616478 -12.26824188   0.53585434 -67.68919373 -21.49348831\n",
       "   11.57093811   4.01305819 -27.06964302   0.92559052  20.41955185]\n",
       " [ 34.71361542  54.35791397  15.96804905 -42.35270691 -28.80783081\n",
       "  -18.14553833  24.23805428 -17.23973465 -46.54365158 -43.37631226]\n",
       " [ 46.36400604  31.58365059  18.92700958 -30.7182045  -49.59332657\n",
       "  -45.20763397 -24.84024811 -55.39533615 -34.26000214  32.4546051 ]\n",
       " [-20.60396385   0.76935959  24.81659317  26.27899933   4.03441715\n",
       "  -10.80195999  42.92971802  19.94259644  -2.52556801  23.67677498]\n",
       " [ 50.46257401  28.26952553  -5.46770954  -5.81841707 -30.15883255\n",
       "   -9.34674835  10.22327137 -14.16289711  47.25200272  24.09783554]\n",
       " [-30.05833817  -5.08619976  20.60229874  -6.65428543  -5.36405182\n",
       "    5.42151403   2.30531597  17.56565094 -40.39870453  38.98799515]\n",
       " [ 56.66824722 -27.6525116  -17.17348099  13.14753532  52.28789902\n",
       "   17.3592453    0.25780487  16.47946548   5.21161556  22.23991776]\n",
       " [ 26.29082489 -36.48064041  -7.5071764   12.80877304 -23.1267128\n",
       "  -26.38866234   6.52116108  31.22356033  22.45219994  44.33024216]\n",
       " [-29.13835526 -57.57143402 -47.81112289  -0.62804413  36.45204544\n",
       "   32.02845764 -26.18133926 -48.31118393 -16.39954376 -11.89389992]\n",
       " [-80.61982727 -44.85277939 -20.20269012 -26.36059189   5.94626808\n",
       "    5.26630211   4.30081272 -25.52010155 -13.16218948  22.30298615]\n",
       " [-22.57714081  18.89765549   6.74625206 -18.11871338 -42.76538849\n",
       "  -16.17328644 -15.79489422 -24.15514374  -5.60876608  -7.23611641]\n",
       " [ 22.26163101 -52.66227341  25.16307831 -23.24165535 -12.70235062\n",
       "   -3.24420357  43.06351471  11.95438862 -19.49994659 -38.48927307]\n",
       " [ -8.72135735  68.31987     13.24241543 -10.74293327  16.21796608\n",
       "   24.31778526  12.47184181 -19.18007469 -36.89630127 -31.70481491]\n",
       " [  5.21699047 -19.49153709 -17.44350052  -2.21118832  -6.6988616\n",
       "   28.78797531  21.85299301   4.63231087  12.76736546   5.52905083]\n",
       " [ 42.5991745  -11.40336227  -7.30252838  14.8870945  -11.30088615\n",
       "  -53.01041412  19.74974632 -18.36275482  10.91544628 -25.06833076]\n",
       " [  2.78900242  80.15345764  -9.82667542  -6.78195953  23.90253067\n",
       "  -20.77098656 -35.17940521  68.39820099   9.514884   -32.03157425]\n",
       " [ 12.58091927  -2.29640794 -30.87487793  -1.95402718  22.86381149\n",
       "   47.83123016  30.3504715    1.83633804 -28.79421616  -7.21063137]\n",
       " [  9.47898865 -17.04598999 -23.80166626 -28.68018532  11.98054218\n",
       "    7.02617788 -11.83446884  12.88232422  -9.55357265  49.07777405]\n",
       " [  1.16630936  53.26164627  22.99323654 -17.09260941 -22.90193176\n",
       "  -11.70051003  51.11965942 -15.73570538  29.62761307  63.02501297]\n",
       " [ 43.17932129 -10.71578026   7.85956621  55.82442474  14.43248177\n",
       "   22.10777855  20.41498184   8.73642921  26.55076981 -36.4226532 ]\n",
       " [ 24.1710453  -16.91485596  42.98093033  -1.89508057  12.02103043\n",
       "   21.31008148  27.70593643  -1.45427132  -5.41493034 -36.58139801]\n",
       " [-21.3628273   18.14542198  39.5478363   41.57516479 -41.388134\n",
       "    0.25789165  35.66672897 -17.12145805  -5.79918289 -36.52625656]\n",
       " [ 23.52597427  21.17308044  -7.50497055  37.70436859  17.03738403\n",
       "   64.19674683   1.42749357 -37.35570145 -33.47217941  27.98981094]\n",
       " [  9.94541073  -8.85352707  -7.47168159 -17.18406677  24.65248108\n",
       "   13.69842339  10.93164253  19.73901367 -61.56907272   1.79628658]\n",
       " [  8.98360825   9.96579075   3.63072586 -11.29970932 -17.50099945\n",
       "    2.08184814 -29.82779694 -29.35467529 -16.13315201  38.21807098]\n",
       " [  5.9650383    2.64064121 -29.64088058  -0.62787724  13.21109676\n",
       "   37.99797058  17.504673    39.78633499  -9.56526279  63.14664459]\n",
       " [  3.97496128 -31.31227875  -4.88087654  -7.04897118 -13.29390812\n",
       "   -3.72562551  41.42865372 -40.28088379  13.85244274  -3.83701324]\n",
       " [ 47.72980499 -43.27980042 -10.37712097  -5.82363319 -34.78199768\n",
       "    9.49148464   2.43854189   8.59998322  31.15157318  23.0535202 ]\n",
       " [-36.85308838 -13.07229233  18.31319427  -5.00804615  50.68358612\n",
       "    7.51036453 -38.21800995 -27.4397068   37.5634079  -12.40490341]\n",
       " [ 52.7266922  -14.8620348    8.0932045   -4.83697701 -11.50362206\n",
       "  -13.9412632   23.07042694 -60.83522034  70.54928589  27.3962307 ]\n",
       " [-16.27750587  39.53098679 -11.40305519 -22.04130173  40.82186127\n",
       "   -0.33578873   0.99097252  -2.36281919   5.49310112   4.4945755 ]]\n",
       "<NDArray 64x10 @cpu(0)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data = nd.random_normal(shape = (batch_size, num_inputs))\n",
    "net(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-2.45050359  2.84679461 -1.49613762 ...,  2.60544538 -0.50231242\n",
      "  -1.03592861]\n",
      " [-0.88370675 -0.45881107  0.05414434 ...,  0.72174817  0.26860395\n",
      "  -0.3811571 ]\n",
      " [ 0.83540815 -0.70175529 -0.42438081 ..., -0.6014719  -1.7975539\n",
      "   0.22692102]\n",
      " ..., \n",
      " [-0.42860946  0.60486376 -0.56944102 ..., -1.95245838 -1.35636246\n",
      "   2.20442605]\n",
      " [ 2.75363064  0.37479419  0.01753719 ..., -0.87502569  0.72981817\n",
      "  -0.1443232 ]\n",
      " [-1.05701315 -0.16627616 -1.39236116 ..., -1.27199733  0.26171252\n",
      "   1.0708195 ]]\n",
      "<NDArray 10x784 @cpu(0)>\n",
      "\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(net.weight.data())\n",
    "print(net.bias.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax cross entropy loss. Don't need softmax layer\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define accuracy metric (use mx.metric.Accuracy)\n",
    "def evaluate_accuracy(dataset, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for data, label in dataset:\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        pred = nd.argmax(output, axis=1)\n",
    "        acc.update(preds = pred, labels = label)\n",
    "    return acc.get()[1] # ('accuracy', 0.0953)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.04354345713, Train_acc 0.796716666667, Test_acc 0.8081\n",
      "Epoch 1. Loss: 0.88397498525, Train_acc 0.835866666667, Test_acc 0.8483\n",
      "Epoch 2. Loss: 0.724797618659, Train_acc 0.855033333333, Test_acc 0.8663\n",
      "Epoch 3. Loss: 0.64222012802, Train_acc 0.86535, Test_acc 0.8722\n",
      "Epoch 4. Loss: 0.588804508948, Train_acc 0.872383333333, Test_acc 0.8785\n",
      "Epoch 5. Loss: 0.549831206934, Train_acc 0.8787, Test_acc 0.8836\n",
      "Epoch 6. Loss: 0.520435121195, Train_acc 0.88145, Test_acc 0.8836\n",
      "Epoch 7. Loss: 0.496468897633, Train_acc 0.885483333333, Test_acc 0.8879\n",
      "Epoch 8. Loss: 0.47669046909, Train_acc 0.889116666667, Test_acc 0.8905\n",
      "Epoch 9. Loss: 0.459988649201, Train_acc 0.89105, Test_acc 0.8932\n",
      "--- 180.56279301643372 seconds ---\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for data, label in train_data:\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size) # optimizer needs data size\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % \\\n",
    "          (i, cumulative_loss/num_examples, train_accuracy, test_accuracy))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Use  `gluon.Block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use gluon.Block\n",
    "# Define forward method\n",
    "class MLP(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense0 = gluon.nn.Dense(64) # or self.dense0 = gluon.nn.Dense(64, activation = 'relu')\n",
    "            self.dense1 = gluon.nn.Dense(32)\n",
    "            self.dense2 = gluon.nn.Dense(10)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = nd.relu(self.dense0(x)) \n",
    "        x = nd.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del net\n",
    "net = MLP()\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude = 2.24), ctx = model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (dense0): Dense(None -> 64, linear)\n",
      "  (dense1): Dense(None -> 32, linear)\n",
      "  (dense2): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.00460785 -0.10649152  0.01577078 -0.06557376 -0.33917981 -0.08719788\n",
       "   0.27524695 -0.04114221 -0.12004875 -0.08178334]]\n",
       "<NDArray 1x10 @cpu(0)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data, _ in train_data:\n",
    "    data = data.as_in_context(model_ctx)\n",
    "    break\n",
    "\n",
    "# gluon.Block.__call__(x) is defined\n",
    "# net(data) == net.forward(data)\n",
    "net(data[0:1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# optimizer\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.416298688595, Train_acc 0.93665, Test_acc 0.9355\n",
      "Epoch 1. Loss: 0.181774298451, Train_acc 0.958033333333, Test_acc 0.9547\n",
      "Epoch 2. Loss: 0.131911580285, Train_acc 0.969583333333, Test_acc 0.9633\n",
      "Epoch 3. Loss: 0.104332525383, Train_acc 0.958883333333, Test_acc 0.9476\n",
      "Epoch 4. Loss: 0.0879672529335, Train_acc 0.9711, Test_acc 0.9635\n",
      "Epoch 5. Loss: 0.0759275976141, Train_acc 0.978216666667, Test_acc 0.9702\n",
      "Epoch 6. Loss: 0.0651613882383, Train_acc 0.98485, Test_acc 0.9747\n",
      "Epoch 7. Loss: 0.0571395777437, Train_acc 0.986383333333, Test_acc 0.9737\n",
      "Epoch 8. Loss: 0.050226613236, Train_acc 0.987883333333, Test_acc 0.9742\n",
      "Epoch 9. Loss: 0.0440175151316, Train_acc 0.98975, Test_acc 0.9751\n",
      "--- 195.637433052063 seconds ---\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for data, label in train_data:\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record(): # default : train_mode = True\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % \\\n",
    "          (i, cumulative_loss/num_examples, train_accuracy, test_accuracy))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use  `gluon.nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not need to define class\n",
    "# Sequential itself subclasses Block and maintains a list of '_children'\n",
    "del net\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(64, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(0.2)) # Drop 20% of activations\n",
    "    net.add(gluon.nn.Dense(32, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Block.collect_params of Sequential(\n",
      "  (0): Dense(None -> 64, Activation(relu))\n",
      "  (1): Dropout(p = 0.2)\n",
      "  (2): Dense(None -> 32, Activation(relu))\n",
      "  (3): Dense(None -> 10, linear)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(net.collect_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameter initalize\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude = 2.24), ctx = model_ctx)\n",
    "# loss function\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# optimizer\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.48531457847, Train_acc 0.938616666667, Test_acc 0.94\n",
      "Epoch 1. Loss: 0.227102286805, Train_acc 0.956583333333, Test_acc 0.9513\n",
      "Epoch 2. Loss: 0.181519980558, Train_acc 0.966216666667, Test_acc 0.9616\n",
      "Epoch 3. Loss: 0.15620553847, Train_acc 0.97115, Test_acc 0.9622\n",
      "Epoch 4. Loss: 0.139154357406, Train_acc 0.973283333333, Test_acc 0.9659\n",
      "Epoch 5. Loss: 0.128104955886, Train_acc 0.972183333333, Test_acc 0.9652\n",
      "Epoch 6. Loss: 0.118027490535, Train_acc 0.98055, Test_acc 0.974\n",
      "Epoch 7. Loss: 0.113074361432, Train_acc 0.982133333333, Test_acc 0.9739\n",
      "Epoch 8. Loss: 0.106842348915, Train_acc 0.981766666667, Test_acc 0.9714\n",
      "Epoch 9. Loss: 0.10111082233, Train_acc 0.9837, Test_acc 0.9722\n",
      "--- 218.51742911338806 seconds ---\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for data, label in train_data:\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record(): # default : train_mode = True\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % \\\n",
    "          (i, cumulative_loss/num_examples, train_accuracy, test_accuracy))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenteredLayer(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CenteredLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x - nd.mean(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
